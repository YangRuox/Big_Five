# %%
import pandas as pd
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import TensorDataset, DataLoader
from sklearn.preprocessing import StandardScaler
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.metrics import accuracy_score, f1_score
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
from sklearn.metrics.pairwise import cosine_similarity
from joblib import load
import streamlit as st
import joblib
import plotly.graph_objects as go
from fpdf import FPDF
# %%

   # è¯»å–Excelæ–‡ä»¶
big5_df = pd.read_excel('Job-profile.xlsx', sheet_name='Big Five Domains')

# æå–äº”å¤§ç‰¹è´¨ç‰¹å¾
features = big5_df[['Neuroticism (M)', 'Extraversion (M)', 
                    'Openness (M)', 'Agreeableness (M)', 
                    'Conscientiousness (M)']]
job_names = big5_df['Job'].tolist()

# æ ‡å‡†åŒ–å¤„ç†
scaler = StandardScaler()
scaled_features = scaler.fit_transform(features)


# %%
def augment_data(features, noise_ratio, n_samples_per_job):
    noise_scale = np.std(features, axis=0) * noise_ratio

    X_augmented = []
    y_augmented = []

    for job_idx, original_feature in enumerate(features):
        samples = original_feature + np.random.normal(scale=noise_scale, size=(n_samples_per_job, features.shape[1]))
        X_augmented.append(samples)
        y_augmented.extend([job_idx] * n_samples_per_job)

    X = np.vstack(X_augmented)
    y = np.array(y_augmented)
    return X, y

class JobRecommenderMLP(nn.Module):
    def __init__(self, input_dim, hidden_dim, output_dim):
        super(JobRecommenderMLP, self).__init__()
        self.model = nn.Sequential(
            nn.Linear(input_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, output_dim)
        )
    
    def forward(self, x):
        return self.model(x)


# %%
# æ•°æ®å¢å¼º
X_aug, y_aug = augment_data(scaled_features, noise_ratio=0.1, n_samples_per_job=1000)



# æ•°æ®åˆ’åˆ†
X_train, X_val, y_train, y_val = train_test_split(X_aug, y_aug, test_size=0.25, random_state=42)

train_dataset = TensorDataset(torch.tensor(X_train, dtype=torch.float32), torch.tensor(y_train, dtype=torch.long))
val_dataset = TensorDataset(torch.tensor(X_val, dtype=torch.float32), torch.tensor(y_val, dtype=torch.long))

train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)

# æ¨¡å‹å®šä¹‰ï¼ˆä½ å·²æœ‰ï¼‰
input_dim = X_aug.shape[1]
hidden_dim = 128
output_dim = len(job_names)
model = JobRecommenderMLP(input_dim, hidden_dim, output_dim)

# æŸå¤±å‡½æ•° & ä¼˜åŒ–å™¨
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)

# è®­ç»ƒå‚æ•°
n_epochs = 100
patience = 3
best_val_loss = float('inf')
wait = 0

train_loss_list, val_loss_list = [], []
train_acc_list, val_acc_list = [], []
train_f1_list, val_f1_list = [], []

for epoch in range(n_epochs):
    model.train()
    train_preds, train_targets = [], []
    total_train_loss = 0

    for batch_X, batch_y in train_loader:
        optimizer.zero_grad()
        output = model(batch_X)
        loss = criterion(output, batch_y)
        loss.backward()
        optimizer.step()
        total_train_loss += loss.item()

        pred_labels = torch.argmax(output, dim=1).detach().cpu().numpy()
        train_preds.extend(pred_labels)
        train_targets.extend(batch_y.detach().cpu().numpy())

    avg_train_loss = total_train_loss / len(train_loader)
    train_acc = accuracy_score(train_targets, train_preds)
    train_f1 = f1_score(train_targets, train_preds, average='macro')

    # éªŒè¯é˜¶æ®µ
    model.eval()
    val_preds, val_targets = [], []
    total_val_loss = 0

    with torch.no_grad():
        for batch_X, batch_y in val_loader:
            output = model(batch_X)
            loss = criterion(output, batch_y)
            total_val_loss += loss.item()

            pred_labels = torch.argmax(output, dim=1).detach().cpu().numpy()
            val_preds.extend(pred_labels)
            val_targets.extend(batch_y.detach().cpu().numpy())

    avg_val_loss = total_val_loss / len(val_loader)
    val_acc = accuracy_score(val_targets, val_preds)
    val_f1 = f1_score(val_targets, val_preds, average='macro')

    # ä¿å­˜è®°å½•
    train_loss_list.append(avg_train_loss)
    val_loss_list.append(avg_val_loss)
    train_acc_list.append(train_acc)
    val_acc_list.append(val_acc)
    train_f1_list.append(train_f1)
    val_f1_list.append(val_f1)

    print(f"Epoch {epoch+1}/{n_epochs} | "
          f"Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f} | "
          f"Train Acc: {train_acc:.4f}, Val Acc: {val_acc:.4f} | "
          f"Train F1: {train_f1:.4f}, Val F1: {val_f1:.4f}")

    # Early stopping
    if avg_val_loss < best_val_loss:
        best_val_loss = avg_val_loss
        best_model_state = model.state_dict()
        wait = 0
    else:
        wait += 1
        if wait >= patience:
            print("Early stopping triggered.")
            break

# åŠ è½½æœ€ä½³æ¨¡å‹
model.load_state_dict(best_model_state)

# %%
# Loss
plt.figure(figsize=(12,4))
plt.subplot(1,3,1)
plt.plot(train_loss_list, label='Train Loss')
plt.plot(val_loss_list, label='Val Loss')
plt.title("Loss Curve")
plt.xlabel("Epoch")
plt.ylabel("Loss")
plt.legend()

# Accuracy
plt.subplot(1,3,2)
plt.plot(train_acc_list, label='Train Acc')
plt.plot(val_acc_list, label='Val Acc')
plt.title("Accuracy Curve")
plt.xlabel("Epoch")
plt.ylabel("Accuracy")
plt.legend()

# F1
plt.subplot(1,3,3)
plt.plot(train_f1_list, label='Train F1')
plt.plot(val_f1_list, label='Val F1')
plt.title("F1 Score Curve")
plt.xlabel("Epoch")
plt.ylabel("F1 Score")
plt.legend()

plt.tight_layout()
plt.show()


# %%
job_codes = big5_df['Code'].tolist()

from sklearn.metrics.pairwise import cosine_similarity
similarity_matrix = cosine_similarity(scaled_features)
# similarity_matrix.shape = (263, 263)


def recommend_jobs(user_big5_scores, model, similarity_matrix, top_k=10):
    model.eval()
    with torch.no_grad():
        # æ ‡å‡†åŒ–ç”¨æˆ·è¾“å…¥
        user_scaled = scaler.transform([user_big5_scores])
        user_tensor = torch.tensor(user_scaled, dtype=torch.float32)

        # MLP è¾“å‡º logits
        logits = model(user_tensor).numpy().flatten()

        # è®¡ç®— similarity-aware scoreï¼ˆé€»è¾‘è¾“å‡º Ã— ç›¸ä¼¼åº¦ï¼‰
        match_score = similarity_matrix @ logits

        # å– Top-k
        top_indices = np.argsort(match_score)[-top_k:][::-1]
        top_jobs = [(job_codes[i], job_names[i], match_score[i]) for i in top_indices]  # åŠ ä¸Šä»£ç 

        return top_jobs

user_input = [51.46,	51.17,	46.09,	52.15,	49.41]

recommendations = recommend_jobs(user_input, model, similarity_matrix, top_k=10)

for i, (code, job, score) in enumerate(recommendations):
    print(f"{i+1}. {code} - {job} (score: {score:.2f})")


# %%
# å‡è®¾ä½ çš„æ¨¡å‹æ˜¯ model
torch.save(model.state_dict(), "your_model.pth")


# %%
# å‡è®¾ä½ å·²ç»å¯¹ç‰¹å¾ä½¿ç”¨äº† scaler
scaler = StandardScaler()
scaler.fit(X_train)  # è¿™é‡Œ X_train æ˜¯ä½ è®­ç»ƒæ•°æ®çš„ç‰¹å¾
joblib.dump(scaler, "your_scaler.pkl")


# %%
# å‡è®¾ job_names å’Œ job_codes æ˜¯ä½ æ‹¥æœ‰çš„èŒä¸šåç§°å’Œä»£ç åˆ—è¡¨
job_names = big5_df['Job'].tolist() # ä½ çš„èŒä¸šåç§°åˆ—è¡¨
job_codes = big5_df['Code'].tolist() # ä½ çš„èŒä¸šä»£ç åˆ—è¡¨

# ä¿å­˜ä¸º numpy æ–‡ä»¶
np.save("job_names.npy", job_names)
np.save("job_codes.npy", job_codes)


# %%
# å‡è®¾ scaled_features æ˜¯ä½ çš„èŒä¸šç‰¹å¾ç»è¿‡æ ‡å‡†åŒ–åçš„æ•°æ®
 # å‡è®¾ job_features æ˜¯ä½ æœªæ ‡å‡†åŒ–çš„èŒä¸šç‰¹å¾

# ä¿å­˜æ ‡å‡†åŒ–åçš„èŒä¸šç‰¹å¾
np.save("scaled_features.npy", scaled_features)


# %%


# ä¿å­˜ç›¸ä¼¼åº¦çŸ©é˜µ
np.save("similarity_matrix.npy", similarity_matrix)


# %%



# å®šä¹‰ä½ çš„æ¨¡å‹æ¶æ„
class JobRecommenderMLP(nn.Module):
    def __init__(self, input_dim, hidden_dim, output_dim):
        super(JobRecommenderMLP, self).__init__()
        self.model = nn.Sequential(
            nn.Linear(input_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, output_dim)
        )
    
    def forward(self, x):
        return self.model(x)


@st.cache_data
def load_data():
    mean_norms = pd.read_csv('meanNorms.tsv', sep='\t')
    sd_norms = pd.read_csv('sdNorms.tsv', sep='\t')
    questions = pd.read_csv('questions.tsv', sep='\t')
    weights = pd.read_csv('weightsB5.tsv', sep='\t')
    return mean_norms, sd_norms, questions, weights
   
mean_norms, sd_norms, questions, weights = load_data()

# 74é“é¢˜ 
items = list(questions['en'])

response_dict = {}

with st.form("bfi_form"):
   # æ˜¾ç¤ºè¡¨å•
   st.title("ğŸ” Big Five Personality Test + Career Recommender")
   st.markdown("Please rate the following statements based on your true feelings: **1 (Strongly Disagree) to 5 (Strongly Agree)**")
  
   # æ€§åˆ«å¹´é¾„é€‰æ‹©
   gender = st.selectbox("Select your gender:", ["Female", "Male"])
   age = st.number_input("Enter your age:", min_value=18, max_value=70, value=25)
   
   if age < 18 or age > 70:
       st.warning("Sorry, your age does not meet the requirements.")
       st.stop()  # æäº¤è¡¨å•ä¹‹å‰åœæ­¢æ‰§è¡Œ

   if "age" not in st.session_state:
        st.session_state.age = 25  # é»˜è®¤å¹´é¾„

   if "gender" not in st.session_state:
        st.session_state.gender = "Female"  # é»˜è®¤æ€§åˆ«
    
   st.subheader("ğŸ‘‡ Please fill in your questionnaire answers")

    # é—®é¢˜çš„æ»‘åŠ¨æ¡
    for i, q in enumerate(questions["en"]):
        key = f"q{i}"
        response_dict[key] = st.slider(
            q,
            min_value=1, max_value=6,
            value=st.session_state.get(key, 3),
            key=key
        )

    # æ£€æŸ¥æ˜¯å¦å®Œæˆæ‰€æœ‰é—®é¢˜
    if all(v is not None for v in response_dict.values()):
        submitted = st.form_submit_button("ğŸ¯ Submit and Recommend Careers")
    else:
        submitted = False
        st.warning("Please answer all questions before submitting.")  # æç¤ºç”¨æˆ·å›ç­”æ‰€æœ‰é—®é¢˜

if submitted:
    st.session_state.age = age
    st.session_state.gender = gender

   # åˆ†ç»„
    if gender == "Female":
        normgroup = 1 if age < 35 else 2
    else:
        normgroup = 3 if age < 35 else 4

    progress = st.progress(0, text="â³ Processing...")
    for i in range(60):
        time.sleep(0.005)
        progress.progress(i + 1)
 
    # Step 1: è·å– norm Î¼ å’Œ Ïƒ
    mu = mean_norms[mean_norms['group'] == normgroup].iloc[0, 1:].values  # è·³è¿‡ group åˆ—
    sigma = sd_norms[sd_norms['group'] == normgroup].iloc[0, 1:].values

    # Step 2: ç”¨æˆ·å›ç­”è½¬ numpy
    responses = np.array([response_dict[f"q{i}"] for i in range(len(questions))])

    # Step 3: è®¡ç®— Z 
    Z = (responses - mu) / sigma

    # Step 4: åŠ æƒæ±‚ Big Five å¾—åˆ†ï¼ˆweightsB5 ä¸º 74x5ï¼ŒT ä¸º 74x1ï¼Œè¾“å‡ºä¸º 5x1ï¼‰
    big5_scores = np.dot(Z, weights.values)  # shape: (5,)
    T_scores = 10 * big5_scores + 50 

    # Step 5: æ ‡å‡†åŒ–ï¼ˆç”¨ä½ çš„ scalerï¼‰
    scaled_input = scaler.transform([T_scores])

    trait_names = ["Neuroticism", "Extraversion", "Openness", "Agreeableness", "Conscientiousness"]
    # é—­åˆé›·è¾¾å›¾æ•°æ®ï¼ˆèµ·ç‚¹å’Œç»ˆç‚¹ä¸€è‡´ï¼‰
    radar_values = list(T_scores) + [T_scores[0]]
    radar_labels = trait_names + [trait_names[0]]
    fig = go.Figure()

    fig.add_trace(go.Scatterpolar(
        r=radar_values,
        theta=radar_labels,
        fill='toself',
        name='Your Big Five T Scores',
        line=dict(color='royalblue')
    ))

    fig.update_layout(
        polar=dict(
            radialaxis=dict(visible=True, range=[0, 100], tickfont=dict(size=10)),
        ),
        showlegend=False,
        title="ğŸ§¬ Your Big Five Personality Profile (T scores)"
    )

    st.plotly_chart(fig)
    

    # Step 6: æ¨¡å‹é¢„æµ‹
    with torch.no_grad():
        input_tensor = torch.tensor(scaled_input, dtype=torch.float32)
        logits = model(input_tensor).numpy().flatten()
        scores = similarity_matrix @ logits
        top_indices = np.argsort(scores)[-10:][::-1]  # å‰10ä¸ªæ¨è
        bottom_indices = np.argsort(scores)[:10]    # æœ€ä¸æ¨èçš„10ä¸ª

        st.subheader("ğŸ§  Recommended Careers Top-10")
        for rank, idx in enumerate(top_indices, 1):
            st.write(f"NO.{rank} - {job_names[idx]}")

        st.subheader("ğŸ˜¬ Least Recommended Careers Bottom-10")
        for rank, idx in enumerate(bottom_indices, 1):
            st.write(f"NO.{rank} - {job_names[idx]}")

        # ğŸŒŸ ç”Ÿæˆ PDF
    pdf = FPDF()
    pdf.add_page()

    pdf.set_font("Arial", size=12)

    # æ·»åŠ æ ‡é¢˜
    pdf.cell(200, 10, txt="Big Five Personality Test Results", ln=True, align='C')

    # ä¸ªäººä¿¡æ¯
    pdf.ln(10)
    pdf.cell(200, 10, txt=f"Gender: {gender}", ln=True)
    pdf.cell(200, 10, txt=f"Age: {age}", ln=True)

    # Big Five Scores
    pdf.ln(10)
    pdf.cell(200, 10, txt="Big Five Personality Scores (T scores):", ln=True)
    for trait, score in zip(trait_names, T_scores):
        pdf.cell(200, 10, txt=f"{trait}: {score:.2f}", ln=True)

    # æ¨èèŒä¸š
    pdf.ln(10)
    pdf.cell(200, 10, txt="Recommended Careers Top-10:", ln=True)
    for rank, idx in enumerate(top_indices, 1):
        pdf.cell(200, 10, txt=f"{rank}. {job_names[idx]}", ln=True)

    # æœ€ä¸æ¨èèŒä¸š
    pdf.ln(10)
    pdf.cell(200, 10, txt="Least Recommended Careers Bottom-10:", ln=True)
    for rank, idx in enumerate(bottom_indices, 1):
        pdf.cell(200, 10, txt=f"{rank}. {job_names[idx]}", ln=True)

    # ä¿å­˜ PDF æ–‡ä»¶
    pdf_output = "BigFive_Test_Result.pdf"
    pdf.output(pdf_output)

    # æä¾›ä¸‹è½½é“¾æ¥
    with open(pdf_output, "rb") as f:
        st.download_button("Download Your PDF Report", f, file_name=pdf_output)










   

# %%


# %%


# %%


# %%



